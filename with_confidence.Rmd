---
title: "With Confidence"
output:
  html_document:
    df_print: paged
---

```{r setup}
library(purrr)
library(ggplot2)
library(tibble)
library(tidyr)
library(magrittr)
library(ggthemes)
library(dplyr)
library(latex2exp)
```


# Introduction

This workbook shall cover the topic of confidence intervals as used in statistics, in particular regression. By following the code and explanations, the reader shall be able to understand the idea of confidence intervals (for model parameters) and their meaning for predictive models.

## Outline

We first create the *ground truth*, i.e. some we design a well defined linear relationship between a predictor variable and a result variable. This relationship shall then be rediscovered by a linear regression model that is developed afterwards. To this end, we simulate a large sample of value pairs by adding some random noise of defined magnitude to values drawn from the above mentioned true linear functional relationship.

# The true relationship

The following function models the (in reality unknown) true relationship between variables `x` (independent) and `y` (dependent, result):
```{r true_relation}
beta_0 = 50
beta_1 = 16

y <- function(x) beta_0 + beta_1 * x

```

# Simulated measurements
Before we start with our experiments, we make our analysis reproducible by setting the random seed value.
```{r}
set.seed(123456)
```


Now we assume that we take a certain amount of measurements 
```{r}
n_measurements <- 1000

```
and that these measurements are not exact but there is some random error. This error is assumed to follow a normal distribution with mean 0 and constant variance. 
```{r}
mean_error <- 0
var_error <- 100^2

error <- rnorm(n = n_measurements, mean = mean_error, sd = sqrt(var_error))
```
The independent values are taken from the range 0 to 100.
```{r}
range_x <- c(0, 100)

x <- runif(n = n_measurements, min = range_x[1], max = range_x[2])
```
We then get a list of simulated measurements:
```{r}
measurements <- y(x) + error
```

# Statistical analysis
Now we simulate the process of statistical analysis, i.e. we imagine an observer who runs a number of experiments and collects the measurements.

We start simple and assume one experiment:
```{r}

pick_sample <- function(msrmnt, xval, n) {
  picks <- sample(1:length(msrmnt), size = n)
  x_obs <- xval[picks]
  y_obs <- msrmnt[picks]
  
  list(X = x_obs, Y = y_obs)
}

experiment <- partial(pick_sample, msrmnt = measurements, xval = x)

n_obs <- 50

trial <- experiment(n_obs)

x_obs <- trial$X
y_obs <- trial$Y
```

## Linear model of a single experiment
We can now compute a linear model of this single experiment The linear model uses a *least squares* algorithm to compute estimates of the intercept $\beta_0$ and the coefficient $\beta_1$:

```{r}
linmod <- lm(y_obs ~ x_obs)
summary(linmod)
```
### Computing the estimate of coefficients

The above estimates for the coefficients are computed like explained in the book *A Modern Approach to Regression with R*:

```{r}
(beta_1_hat <- sum((x_obs - mean(x_obs)) * (y_obs - mean(y_obs))) / sum((x_obs - mean(x_obs))**2))

(beta_0_hat <- mean(y_obs) - beta_1_hat * mean(x_obs))

```
Now we compute the standard errors using the formulas in the book:

```{r}
residuals <- y_obs - (beta_0_hat + beta_1_hat * x_obs)

sxx <- sum((x_obs - mean(x_obs))**2)
big_s <- sqrt(1 / (n_obs - 2) * sum(residuals**2))

(std_error_beta_0 <- big_s * sqrt(1 / n_obs + (mean(x_obs))**2 / sxx))

(std_error_beta_1 <- big_s / sqrt(sxx))
```

With these quantities, we can compute the confidence intervals for the coefficients. Note that we need to be careful with the meaning of the probability parameter for the `qt` quantile function for the *t*-distribution when computing the lower confidence interval bound (cf. also the documentation to the *qt* function).
```{r}
alpha <- 0.05
(beta_0_hat_lq <- beta_0_hat - qt(alpha / 2, n_obs - 2, lower.tail = FALSE) * std_error_beta_0)
(beta_0_hat_uq <- beta_0_hat + qt(1 - alpha / 2, n_obs - 2) * std_error_beta_0)

(beta_1_hat_lq <- beta_1_hat - qt(alpha / 2, n_obs - 2, lower.tail = FALSE) * std_error_beta_1)
(beta_1_hat_uq <- beta_1_hat + qt(1 - alpha / 2, n_obs - 2) * std_error_beta_1)

```
Compare this to the output generated by R:
```{r}
linmod %>% confint()

```

The numbers match as -- behind the scenes -- `confint` uses the same *t*-distribution based computation for linear models (as explained in the `confint` documentation).


### Plot of measurements and model predictions
```{r}
pf <- tibble(X_OBS = x_obs, Y_OBS = y_obs, Y_FITTED = linmod$fitted.values)

pf %>% ggplot(aes(x = X_OBS)) + geom_point(aes(y = Y_OBS)) + geom_line(aes(y = Y_FITTED)) +
                labs(title = "Model plot", x = "x", y = "y") +
  ggthemes::theme_economist_white()

```

## Models of several experiments

```{r}
values_per_exp <- 50
num_of_experiments <- 20

exps <- rep(values_per_exp, num_of_experiments)

some_exps <- lapply(exps, experiment)

some_models <- some_exps %>% map(~ lm(Y ~ X, data = .x))

coeff_df <- some_models %>% map(~ .x$coefficients) %>% map_dfr(as.list)
names(coeff_df) <- c("beta_0", "beta_1")

```

### Descriptive statistics of the experimentally obtained coefficients
```{r}
summary(coeff_df)
```
```{r}
alpha <- 0.05
(q95_beta_0 <- quantile(coeff_df$beta_0, probs = c(alpha / 2, 1 - alpha / 2)))

(q95_beta_1 <- quantile(coeff_df$beta_1, probs = c(alpha / 2, 1 - alpha / 2)))
```


### Visualization of experimental results

```{r}
coeff_df %>% select(beta_0) %>% 
  ggplot(aes(x = factor(1), y = beta_0)) +
  geom_boxplot() +
  geom_jitter(width = 0.2) +
  geom_hline(yintercept = q95_beta_0[1], colour = "red") +
  geom_hline(yintercept = q95_beta_0[2], colour = "red") +
  labs(title = TeX("Coefficient $\\beta_0$"), x = element_blank(), y = TeX("$\\beta_0")) + 
  ggthemes::theme_economist_white()
```

```{r}
coeff_df %>% select(beta_1) %>% 
  ggplot(aes(x = factor(1), y = beta_1)) +
  geom_boxplot() +
  geom_jitter(width = 0.2) +
  geom_hline(yintercept = q95_beta_1[1], colour = "red") +
  geom_hline(yintercept = q95_beta_1[2], colour = "red") +
  labs(title = TeX("Coefficient $\\beta_1$"), x = element_blank(), y = TeX("$\\beta_1$")) + 
  ggthemes::theme_economist_white()
```

## The role of the size of experiments

In this chapter we evaluate how the least square coefficients develop as the size of experiments increase, i.e. as the number of observations per experiment increase.

```{r}
exp_sizes <- seq(50, 500, 50)

many_exps <- lapply(exp_sizes, experiment)
many_coeffs <- many_exps %>% map(~ lm(Y ~ X, data = .x)) %>% map(~ .x$coefficients) %>% map_dfr(as.list)
many_coeffs$EXP <- 1:length(exp_sizes)
names(many_coeffs) <- c("experiment", "beta_0", "beta_1")

```
```{r}
many_coeffs %>% pivot_longer(cols = c(beta_0, beta_1), names_to = "coefficient", 
                             values_to = "value") %>%
  ggplot(aes(x = experiment, y = value, color = coefficient)) + 
  geom_line() +
  scale_color_discrete(name = "Coefficient", labels = unname(c(TeX("$\\beta_0$"), TeX("$\\beta_1$")))) +
  labs(title = "Coefficients over experiment size", x = "Experiment", y = "Coefficient value")
```


## The role of the replication number



We run an increasing number of replications for an experiment of constant size, e.g. 100 observations.

```{r}
n_obs <- 100

n_reps <- seq(5, 50, 5)

reps <- replicate(n_reps[1], experiment(n_obs), simplify = FALSE)

# to be
```

